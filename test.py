import nltk
from nltk import word_tokenize

sentence = "I am a test"
tokens = word_tokenize(sentence)

print(tokens)
